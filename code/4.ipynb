{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statistics as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.311111</td>\n",
       "      <td>0.531339</td>\n",
       "      <td>0.268364</td>\n",
       "      <td>2.532152</td>\n",
       "      <td>0.086933</td>\n",
       "      <td>15.615486</td>\n",
       "      <td>45.914698</td>\n",
       "      <td>0.996730</td>\n",
       "      <td>3.311015</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>10.442111</td>\n",
       "      <td>5.657043</td>\n",
       "      <td>804.969379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.747595</td>\n",
       "      <td>0.179633</td>\n",
       "      <td>0.196686</td>\n",
       "      <td>1.355917</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>10.250486</td>\n",
       "      <td>32.782130</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.156664</td>\n",
       "      <td>0.170399</td>\n",
       "      <td>1.082196</td>\n",
       "      <td>0.805824</td>\n",
       "      <td>463.997116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.392500</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.995570</td>\n",
       "      <td>3.205000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>411.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>794.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.100000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.997845</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1209.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1597.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1143.000000       1143.000000  1143.000000     1143.000000   \n",
       "mean        8.311111          0.531339     0.268364        2.532152   \n",
       "std         1.747595          0.179633     0.196686        1.355917   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.392500     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.250000        2.200000   \n",
       "75%         9.100000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1143.000000          1143.000000           1143.000000  1143.000000   \n",
       "mean      0.086933            15.615486             45.914698     0.996730   \n",
       "std       0.047267            10.250486             32.782130     0.001925   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             21.000000     0.995570   \n",
       "50%       0.079000            13.000000             37.000000     0.996680   \n",
       "75%       0.090000            21.000000             61.000000     0.997845   \n",
       "max       0.611000            68.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality           Id  \n",
       "count  1143.000000  1143.000000  1143.000000  1143.000000  1143.000000  \n",
       "mean      3.311015     0.657708    10.442111     5.657043   804.969379  \n",
       "std       0.156664     0.170399     1.082196     0.805824   463.997116  \n",
       "min       2.740000     0.330000     8.400000     3.000000     0.000000  \n",
       "25%       3.205000     0.550000     9.500000     5.000000   411.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000   794.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  1209.500000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  1597.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WineQualityDataset = pd.read_csv('../datasets/WineQT.csv')\n",
    "WineQualityDataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Wine Dataset:\n",
      "Size of training set = 925\n",
      "Size of validation set = 103\n",
      "Size of test set = 115\n"
     ]
    }
   ],
   "source": [
    "Wine_X = WineQualityDataset.drop('quality', axis=1)\n",
    "Wine_y = WineQualityDataset['quality']\n",
    "\n",
    "Wine_X_train, Wine_X_test, Wine_y_train, Wine_y_test = train_test_split(Wine_X, Wine_y, test_size=0.1, random_state=42)\n",
    "Wine_X_train, Wine_X_val, Wine_y_train, Wine_y_val = train_test_split(Wine_X_train, Wine_y_train, test_size=0.1, random_state=52)\n",
    "\n",
    "Wine_X_train = Wine_X_train.to_numpy()\n",
    "Wine_y_train = Wine_y_train.to_numpy()\n",
    "Wine_y_train -= 3 # To make the range of values from 0 to 5\n",
    "\n",
    "Wine_X_val = Wine_X_val.to_numpy()\n",
    "Wine_y_val = Wine_y_val.to_numpy()\n",
    "Wine_y_val -= 3 # To make the range of values from 0 to 5\n",
    "\n",
    "Wine_X_test = Wine_X_test.to_numpy()\n",
    "Wine_y_test = Wine_y_test.to_numpy()\n",
    "Wine_y_test -= 3 # To make the range of values from 0 to 5\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Wine_X_train = scaler.fit_transform(Wine_X_train)\n",
    "Wine_X_val = scaler.fit_transform(Wine_X_val)\n",
    "Wine_X_test = scaler.fit_transform(Wine_X_test)\n",
    "\n",
    "\n",
    "print(\"For Wine Dataset:\")\n",
    "print(f\"Size of training set = {len(Wine_X_train)}\\nSize of validation set = {len(Wine_X_val)}\\nSize of test set = {len(Wine_X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, criterion='gini', max_depth=None, random_state=None, classes=None, task_type='classification'):\n",
    "        if task_type == 'classification':\n",
    "            self.tree = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=random_state)\n",
    "        elif task_type == 'regression':\n",
    "            self.tree = DecisionTreeRegressor(criterion=criterion, max_depth=max_depth, random_state=random_state)\n",
    "        self.classes = classes\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.tree.fit(X, y, sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.tree.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba = self.tree.predict_proba(X)\n",
    "        if self.classes is not None and proba.shape[1] != len(self.classes):\n",
    "            # Adjust the probability array to include missing classes\n",
    "            full_proba = np.zeros((proba.shape[0], len(self.classes)))\n",
    "            indices = np.array([np.where(self.classes == c)[0][0] for c in self.tree.classes_])\n",
    "            full_proba[:, indices] = proba\n",
    "            return full_proba\n",
    "        return proba\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return accuracy_score(y, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, max_features='auto'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.max_features == 'auto':\n",
    "            self.max_features = n_features\n",
    "        elif isinstance(self.max_features, float):\n",
    "            self.max_features = int(self.max_features * n_features)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            idxs = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
    "            sample_X, sample_y = X[idxs], y[idxs]\n",
    "\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(sample_X, sample_y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_preds)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best hyperparmeters for the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for n_estimators = 10 and max_depth = 5 is 0.6260869565217392\n",
      "Accuracy for n_estimators = 10 and max_depth = 10 is 0.6086956521739131\n",
      "Accuracy for n_estimators = 10 and max_depth = 20 is 0.6608695652173913\n",
      "Accuracy for n_estimators = 50 and max_depth = 5 is 0.6347826086956522\n",
      "Accuracy for n_estimators = 50 and max_depth = 10 is 0.6173913043478261\n",
      "Accuracy for n_estimators = 50 and max_depth = 20 is 0.6173913043478261\n",
      "Accuracy for n_estimators = 100 and max_depth = 5 is 0.6173913043478261\n",
      "Accuracy for n_estimators = 100 and max_depth = 10 is 0.6260869565217392\n",
      "Accuracy for n_estimators = 100 and max_depth = 20 is 0.6347826086956522\n",
      "Accuracy for n_estimators = 200 and max_depth = 5 is 0.6260869565217392\n",
      "Accuracy for n_estimators = 200 and max_depth = 10 is 0.6347826086956522\n",
      "Accuracy for n_estimators = 200 and max_depth = 20 is 0.6347826086956522\n",
      "\n",
      "Best parameters: {'n_estimators': 10, 'max_depth': 20}\n",
      "Best accuracy: 0.6608695652173913\n"
     ]
    }
   ],
   "source": [
    "n_estimators_options = [10, 50, 100, 200]\n",
    "max_depth_options = [5, 10, 20]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for n_estimators in n_estimators_options:\n",
    "    for max_depth in max_depth_options:\n",
    "        model = RandomForest(n_estimators=n_estimators, max_depth=max_depth)\n",
    "        model.fit(Wine_X_train, Wine_y_train)\n",
    "        preds = model.predict(Wine_X_test)\n",
    "        accuracy = accuracy_score(Wine_y_test, preds)\n",
    "        print(f\"Accuracy for n_estimators = {n_estimators} and max_depth = {max_depth} is {accuracy}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'n_estimators': n_estimators, 'max_depth': max_depth}\n",
    "\n",
    "print()\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada boost and Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AdaBoost:\n",
    "#     def __init__(self, n_estimators=50, learning_rate=1.0):\n",
    "#         self.n_estimators = n_estimators\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.trees = []\n",
    "#         self.tree_weights = []\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples = X.shape[0]\n",
    "#         sample_weights = np.full(n_samples, 1 / n_samples)\n",
    "\n",
    "#         for _ in range(self.n_estimators):\n",
    "#             tree = DecisionTree(max_depth=1)  # Using stumps as weak learners\n",
    "#             tree.fit(X, y, sample_weight=sample_weights)\n",
    "#             predictions = tree.predict(X)\n",
    "\n",
    "#             misclassified = predictions != y\n",
    "#             error = np.sum(sample_weights * misclassified) / np.sum(sample_weights)\n",
    "#             alpha = self.learning_rate * np.log((1 - error) / error)\n",
    "\n",
    "#             sample_weights *= np.exp(alpha * misclassified)\n",
    "#             sample_weights /= np.sum(sample_weights)  # Normalize weights\n",
    "\n",
    "#             self.trees.append(tree)\n",
    "#             self.tree_weights.append(alpha)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "#         final_prediction = np.sign(np.dot(self.tree_weights, tree_preds))\n",
    "#         return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, M):\n",
    "        self.M = M  # Number of trees\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "\n",
    "        N, _ = X.shape\n",
    "        w = np.ones(N) / N\n",
    "\n",
    "        for m in range(self.M):\n",
    "            tree = DecisionTree(max_depth=1)  # Using stumps (depth-1 trees)\n",
    "            tree.fit(X, y, sample_weight=w)\n",
    "            P = tree.predict(X)\n",
    "\n",
    "            err = np.sum(w * (P != y))\n",
    "            alpha = 0.5 * np.log((1 - err) / err)\n",
    "\n",
    "            w = w * np.exp(-alpha * y * P)  # Update weights\n",
    "            w = w / w.sum()  # Normalize weights\n",
    "\n",
    "            self.models.append(tree)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        N, _ = X.shape\n",
    "        FX = np.zeros(N)\n",
    "        for alpha, tree in zip(self.alphas, self.models):\n",
    "            FX += alpha * tree.predict(X)\n",
    "        return np.sign(FX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.017391304347826087\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoost(M=10)  # 10 trees\n",
    "adaboost.fit(Wine_X_train, Wine_y_train)\n",
    "predictions = adaboost.predict(Wine_X_test)\n",
    "accuracy = accuracy_score(Wine_y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
